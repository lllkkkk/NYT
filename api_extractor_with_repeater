__author__ = 'Karthik'
import json
from urllib import request
from bs4 import BeautifulSoup
import requests


class ApiExtractor:
    present_date = "20150101"
    no = None
    urls = None 
    query = None
    begin_date = None
    end_date = None
    f1 = None
    api_url = "http://api.nytimes.com/svc/search/v2/articlesearch.json?"
    api_key = "api-key=cb2a40c7ef36cfa5b8d07d49170cf0b8%3A5%3A72188329"
    page = None
    http_data = None  # Stores the data from the link in byte format
    data_text = None  # stores the converted byte data into text format
    soup_object = None  # Creates the SoupObject from the data_text

    # Constructor for the class
    def __init__(self):
        self.assign_begin_date("20100101")
        self.assign_end_date("20150101")
        self.assign_page_range("0")
        self.repeater()
        # self.construct_url()

    # creating the soup Object to pass
    def soup_object_creator(self, url_from_extract):
        self.http_data = requests.get(url_from_extract)
        self.data_text = self.http_data.text
        self.soup_object = BeautifulSoup(self.data_text)

 # Customize the start and the end date for the DataBase

    def assign_present_date(self, present_date):
        self.present_date = present_date

    def assign_query(self, query):
        self.query = "q="

    def assign_begin_date(self,begin):
        #begin = "20100505"
        self.begin_date =  begin

    def assign_end_date(self,end_date):
        #end_date = "20150712"
        self.end_date =  end_date

    def assign_delimiter(self):
        self.f1 = "f1=web_url"
    #def assign_sort(self,sort):
    #  sort= newest
    #   self.sort = "sort="+sort

    def assign_page_range(self,no):
        #number_range= no
        self.page = "page=" + no

    def construct_url(self):
        urls = self.api_url + "&" + "begin_date=" + self.begin_date + "&" + "end_date=" + self.end_date + "&" + self.page + "&" + self.api_key
        self.extract(urls)

    def extract(self, url):
        try:
            rewuest = request.urlopen(url)
            tostring = rewuest.read().decode('utf-8')
            jobj = json.loads(tostring)
    #     print(jobj)
            for item in jobj["response"]["docs"]:
                print(item["web_url"])
                #article_extractor(item["web_url"])
                """filename = 'data.txt'
                with open(filename) as f:
                    data = f.readlines()
                    f = open(filename)
                    data = f.readlines()
                    f.close()"""

            for item in jobj["response"]["docs"]:
                pd = item["pub_date"]
                pud_date = pd[:10]
                present_date_st = pud_date.replace("-","")
                self.assign_present_date(present_date_st)

        except ValueError as e:
            print("Error occurred at the Extraction Function call :")

    def repeater(self):
        for no in range(1,100):
            no = str(no)
            self.assign_page_range(no)
            if int(self.present_date) >= int(self.begin_date):
            #print (self.begin_date)
                self.construct_url()

            """
            if date < int(self.begin_date)
                self.assign_end_date(str(date))
            ur = self.construct_url()
            #self.extract(ur)
            """
test = ApiExtractor()

#ur = test.construct_url
#jsonObj = test.extract(ur)
'''
link = 'http://www.somesite.com/details.pl?urn=2344'
f = urllib.urlopen(link)
myfile = f.readline()
print myfile
'''

# Extract function base class
""""
def extract(self, urls):
            #rewuest = request.urlopen(self.api_url+"&"+self.api_key)
            rewuest = request.urlopen(urls)

        #    req_jsondata = request.urlopen(rewuest)
            tostring = rewuest.read().decode('utf-8')
            jobj = json.loads(tostring)
            for item in jobj["response"]["docs"]:
                print(item["web_url"])
           # self.repeater()

"""
